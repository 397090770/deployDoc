# Build-and-install-Spark

要想深入理解spark，首先要对spark的理论有一定的了解，这边我推荐去官网上看spark作者相关论文，对spark的设计原理讲的比较清楚，之后就是需要如何使用spark跑一些简单的程序了，所以，这章我们主要讲解如何将spark搭建起来。按着一下的步骤可以轻松的搭建spark单机版，篇末会讲解如何搭建spark集群，以及构建HA spark集群。

###搭建无hadoop单机版spark
 - 安装linux操作系统（我是用的是ubuntu12.04-desktop或则ubuntu12.04-server版本（服务器））
 - 安装好linux操作系统之后，进入操作系统，
 
 在/etc/hostname 文件里将主机名更改
 修改为：master（先以这台机器搭建单机版本，之后会有slave几点搭建集群）

 - 将自己的ip设置为静态ip
 
  在/etc/network/interfaces 文件里添加
  这里需要根据自己的网络配置信息进行相应的更改

```
    auto eth0    //多网卡的机器根据网卡联网信息填写ethx
    iface eth0 inet static
    address 192.168.1.98 
    netmask 255.255.255.0
    gateway 192.168.1.1
    network 192.168.1.0
    broadcast 192.168.1.255
    dns-nameservers 202.96.209.133 //这里设置成192.168.1.1即可 
    编辑好之后 执行指令：sudo /etc/init.d/networking restart重启网络，使静态ip生效
```
 - 编辑/etc/hosts
  
   修改主机名与 ip 地址映射

   192.168.1.98 master
 - 创建用户和用户组
 
 这边大家可以随便使用用户和用户组
 
```
    sudo addgroup havstack (我是用的是havstack,大家可以改成hadoop或则其他)添加用户组
    sudo adduser --ingroup havstack havstack
    为用户havstack赋予sudo操作权限
    编辑/etc/sudoers编辑文件，
    在root ALL=(ALL)ALL行下添加havstack ALL=(ALL)ALL
    sudo chmod +w /etc/sudoers
    sudo vim /etc/sudoers //之后添加上面这行
    sudo chmod -w /etc/sudoers
    之后切换到havstack用户下。
```

 - 配置ssh
 
```
    sudo apt-get install openssh-server
    sudo apt-get install openssh-client
    安装完ssh之后，配置本机无密码登入
    ssh-keygen -t rsa -P ""
    cat ～/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    ssh localhost验证释放可以免密码登录localhost
```
 - 安装配置jdk

```
    下载jdk(最好是1.7版本)
    sudo mkdir /usr/lib/jvm
    sudo tar -zxvf jdk-7u60-linux-x64 (根据自己的jdk版本解压)
    sudo mv jdk1.7.0_60 /usr/lib/jvm
    之后配置bash
    sudo vim /etc/profile
    添加如下配置
    export JAVA_HOME=/usr/lib/jvm/jdk1.7.0_60
    export CLASS_PATH=$JAVA_HOME/lib:$JAVA_HOME/jre/lib
    export PATH=$PATH:$JAVA_HOME/bin
```

 - 安装maven

```
    sudo apt-get install maven
    
    关于maven的相关配置我想单独写一篇文章详细介绍如何配置使用oschina依赖库
```

 - 安装sbt

```
    下载sbt.tgz
    tar -zxvf sbt.tgz
    将sbt路径加入到/etc/profile
    export SBT_HOME=/home/havstack/sbt
    export PATH=$PATH:$SBT_HOME/bin:$JAVA_HOME/bin
```
 - 基本的环境搭建好之后就编译自己的spark吧，(这一步完全可以不用，到官网下载pre-built的spark版本)

```
    下载好源码之后
    直接调用其脚本程序./make-distribution.sh 就可以生成默认的spark编译好的版本
```





