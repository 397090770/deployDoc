# 异常总结

###1.Spark和hadoop依赖冲突
最近在实践项目遇到了问题，hadoop使用的是2.6.0(2.2.0也测试过)，spark使用的1.2.1编译为支持hadoop2.6.0,在intellij中使用下面依赖：
```
<dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${spark.scala.version}</artifactId>
            <version>1.2.1</version>
            <exclusions>
                <exclusion>
                    <groupId>javax.servlet</groupId>
                    <artifactId>*</artifactId>
                </exclusion>
            </exclusions>
</dependency>
```
在IDE中可以正常执行并产生结果，但是当将程序进行打包的之后提交到集群中的时候出现下面错误（ps:Spark处理的数据源来自hdfs）
```
Exception in thread "main" java.lang.VerifyError: class org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
```
查看了很多资料，到spark User List中也看到了类似的问题，问题就是：hadoop依赖的protobuf是2.5.0而Spark依赖的是2.4.1.这个可以下载Spark源码打开工程的父Pom.xml文件，查看到protoc的版本是2.4.1.于是将其改成2.5.0，然后重新编译，重新搭建集群，问题解决。可能会有其他问题,[Click here]，上面链接有大神们讨论可能出现的问题，但是不管怎么样，我的问题解决了，如果再出现问题，再补充。

###2.maven依赖问题
maven问题一直是一个迷，没有对maven进行过深入研究，出现过很多问题，之前都是解决了就没有总结，导致之后又出问题了还得从新再来。这里开始记录一些问题，极少成多把。

**本地依赖问题**

        有些时候我们依赖的jar包可能是本地jar包，例如google管理的一些jar包，可以通过将其下载到本地，然后加入到maven中去，或者自己编译的一些文件例如修改过的spark源码，将生成的sparkjar加入到项目中，而不使用maven依赖。这样我们具有一下方法解决：
```
<dependency>
    <groupId>com.xuggle</groupId>
    <artifactId>xuggle-xuggler</artifactId>
    <version>5.4</version>
    <scope>system</scope>
    <systemPath>${project.basedir}/lib/xuggle-xuggler-5.4.jar</systemPath>
</dependency>
```
如上所示，将本地的jar包通过system scope将其引入。这种方法在ide中执行没有问题，可以找到依赖，但是如果通过maven shade 打包的时候会发现，并没有将其打入执行jar包中
```
<resources>
    <resource>
        <filtering>true</filtering>
         <directory>${project.basedir}/lib</directory>
         <includes>
            <include>xuggle-xuggler-5.4.jar</include>
         </indelucs>
    </resource>
</resources>
```
上述方法只是将这个jar包打进去了，但是并不能解决问题，还是会抛出异常
```
Lost task 4.0 in stage 0.0 (TID 4, slave2): java.lang.NoClassDefFoundError: com/xuggle/xuggler/IContainer
	at org.nita.cipher.hadoop.XugglerReader.<init>(XugglerReader.java:129)
	at org.nita.cipher.hadoop.VideoRecordReader.initialize(VideoRecordReader.java:104)
```
解决方案是：
```
mvn install:install-file -Dfile=xuggle-xuggler-5.4.jar -DgroupId=com.xuggle -DartifactId=xuggle-xuggler -Dversion=5.4 -Dpackaging=jar
打印：
Installing /home/havstack/git/cipher/core/lib/xuggle-xuggler-5.4.jar to /home/havstack/.m2/repository/com/xuggle/xuggle-xuggler/5.4/xuggle-xuggler-5.4.jar
```
使用上诉指令将本地的包打进库里去，然后在项目中使用
```
<dependency>
    <groupId>com.xuggle</groupId>
    <artifactId>xuggle-xuggler</artifactId>
    <version>5.4</version>
</dependency>
```
将jar包引入，这样利用maven插件打包就能将其打入执行包中。后续问题会陆续补充

[Click here]:http://apache-spark-user-list.1001560.n3.nabble.com/Error-reading-HDFS-file-using-spark-0-9-0-hadoop-2-2-0-incompatible-protobuf-2-5-and-2-4-1-tc2158.html#a2807
