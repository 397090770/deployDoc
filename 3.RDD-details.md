# 细说RDD
要了解Spark RDD是什么东西，其内部怎么表示的？首先我们需要了解一下RDD的概念，然后需要知道如何使用RDD，如何利用它来做一些具体的事情，最后我们从源码的角度去分析RDD到底是一个什么东西。如果对RDD进行了深入的分析，那么之后的Spark内核，Spark流计算、图计算、机器学习等方面的深入研究会如鱼得水。本人并非大神，只是对Spark和Hadoop接触的时间比较长，自然熟悉一些，写文章只是给自己一个学习的一个记录和总结，如果能帮助到一些同学的话，自然会十分高兴，其中错误也肯定很多，希望大家多指出。其中 参考了[SparkInternals]、Spark大数据处理一书。


###RDD简介
首先我推荐大家看一下Spark作者写的一篇文章：[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]。该文章详细的阐述了RDD到底是一个什么东西，并介绍了RDD和DSM的区别，同时介绍了RDD的一些操作并且如何运用到机器学习、日志统计等应用中去。

**RDD简介**

RDD（Resilient Distributed Datasets），它是一个非常重要的分布式数据结构，它是逻辑集中的实体，在集群中的多台机器上进行了数据分区，通过对多台机器上不同RDD分区控制，可以减少机器之间的数据重排，能够充分的利用数据的本地性进行任务的计算，提高集群的资源利用率。Spark提供了“partitionBY”运算符，能够通过集群中多台机器之间对原始RDD进行数据再分配来创建一个新的RDD。RDD是Spark的核心数据结构，通过RDD的依赖关系形成Spark的调度顺序。通过对RDD的操作形成整个Spark程序。其实用一句话来对RDD进行总结就是：RDD记录了需要处理的数据（这些数据可能存在不同的节点，在逻辑上使用partition进行分割，在物理上使用block表示），同时记录了需要对数据进行的操作，就是用什么函数对其进行操作，当然还记录了其他的一些信息。
 - RDD的创建方式

   通过去看SparkContext文件可以发现，基本上有两种方式创建RDD，第一种是：从Scala集合中构建RDD；第二种是：从Hadoop支持的文件系统中创建RDD。说白了，RDD的创建就是将数据源引入到集群，不然Spark还进行什么操作？PS：这里没有涉及到SparkStreaming或则Spark SQL等构建如何进行数据的引入，它们都有自己的方式构建RDD，但是这些已经在源码中实现好了，暴露给我们的都是一些简单的接口。
 - RDD的两种操作算子
   
  对于RDD有两种计算操作算子：Transformation（转换操作）和Action（行动）

  Transformation操作：其实就是将一个操作的结果传递给下一个操作进行计算，以此类推从而将用户需要的各种逻辑操作得以体现，但是这里的操作并不会触发Spark进行计算，只有在最后加上Action操作才会触发计算，其实很简单，你对数据各种操作之后，你总归需要将数据进行保存或则统计打印，而这些操作就是Action操作。没有Action操作，Spark就认为这是没有意义的操作。
  
  Action操作：用来触发Spark提交作业，并将数据输出。
  
 - RDD的重要内部属性（其实就是RDD类中的几个重要的方法和属性）

    （1）分区列表（2）计算每个分片的函数（3）对父RDD的依赖列表（4）对Key-alue对数据类型RDD的分区器，控制分区策略和分区数（5）每个数据分区的地址列表
    
###RDD与分布式共享内存的异同
> RDD是一种分布式的内存抽象，![图](https://github.com/gjhkael/deployDoc/blob/master/image/snapshot60.png) 列出了RDD与分布式共享内存（Distributed Shared Memory,DSM）的对比.在DSM系统中，应用可以向全局地址空间的任意位置进行读写操作。DSM是一种通用的内存数据抽象，但这种通用性的同时也使其在商用集群上实现有效的容错性和一致性更加困难。

> RDD与DSM主要区别在于，不仅可以通过批量转换创建RDD，还可以对任意内存位置读写。RDD限制应用执行批量写操作，这样有利于实现有效的容错。特别是由于RDD可以使用Lineage来恢复分区，基本没有检查点开销，失效时只需要重新计算丢失的那些RDD分区，就可以在不同节点上并行执行，而不需要回滚整个程序。

###Spark的数据存储
RDD可以被抽象的认为是一个大的数组，但是这个数组是分布在集群上的。逻辑上RDD的每个分区叫一个Partition。

> 在Spark的执行过程中，RDD经历一个个的Transformation算子之后，最后通过Action算子进行触发计算操作。逻辑上每经历一次变换，就会将RDD转换为一个新的RDD，RDD之间通过Lineage产生依赖关系，这个关系在容错中具有重要的作用。变换的输入和输出都是RDD。RDD会被划分成很多的分区分布到集群的多个节点上。分区是个逻辑概念，变换前后的新旧分区在物理上可能是同一块内存存储。举例如果是从HDFS上读取文件过来的话，那么默认一个文件就是一个RDD，但是如果这个文件比较大，那么它在HDFS上就会分布式的存储到集群的各个节点，并以64M为物理块存储，那么该RDD默认会将该文件的物理块数量作为RDD的逻辑分区数量既Partition。

> 如下图所示为RDD数据管理分布图，![图](https://github.com/gjhkael/deployDoc/blob/master/image/rdd.png) 
在上图中，RDD1具有五个分区，并且分布在四个节点上，RDD2含有三个分区，分布在三个节点上。在物理上，RDD对象实质上是一个元数据结构，存储着Block、Node等的映射信息，以及其他的一些元数据信息。一个RDD就是一组分区，在物理数据存储上，RDD的每个分区对应的就是一个Block，而物理存储的Block可以存储在内存，当内存不够的时候可以存储在磁盘上。

>每个Block中存储着RDD中的一部分数据，而对RDD的操作也可以是针对Block的操作例如：mapPartitions()也可以是针对这一整个RDD数据进行操作如：map()操作。

###RDD算子
要想让一个Spark可以运行起来，首先需要有一些RDD算子可以用来加载数据的，然后再对数据进行一些转换操作，最后需要将数据进行打印或则存储出来这就需要action操作。所以一个整个完整的Spark程序是：加载数据->转换数据->保存结果。如下图所示：一个完整的Spark程序与数据相关的一些装换。
![图](https://github.com/gjhkael/deployDoc/blob/master/image/dataflow.png)

算子的分类，我在这边大致将RDD所有的算子分为4大类

- 加载数据类
- Value数据类型的Transformation算子
- Key-Value数据类型的Transformation算子
- Action类型的算子

**（1）加载数据类**

>这些方法主要是在SparkContext文件中

>加载数据类的大致可以分为两个部分：从scala集合中构建RDD和从文件系统中构建RDD(PS:一下程序在我的SpaarkExample中都有，直接下载运行即可)。

先看第一种：**从scala集合中构建RDD**

```scala

package org.nita.sparkExample.spark
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import scala.collection.mutable.LinkedList
import scala.collection.SortedMap
import scala.collection.mutable.ListBuffer
import scala.collection.immutable.Queue
object IntersectionTest {
  def main(args: Array[String]) {
      val sparkConf = new SparkConf().setAppName("IntersectionTest").setMaster("local[4]")
      .set("spark.driver.allowMultipleContexts", "true")   //忽略此参数，目前还不支持多个context对象
      val sc = new SparkContext(sparkConf)
      val a=sc.parallelize(List(1,2,3,4,5,6),3)       //3为partition的个数，此方法为创建RDD的其中一个方法
      //val a=sc.makeRDD(List(1,2,3,4,5,6),3)          //同上,只要是继承至Seq序列的都可以作为第一个参数来构造RDD
      
      val b=sc.parallelize(List(1,2,5,6), 2)         //2为partition的个数
      var r=a.intersection(b)          //求a和b的交集
      a.foreach(x=>println(x))          //将每个数打印出来
      a.foreachWith(i=>i)((x,i)=>println("[aIndex"+i+"]"+x))  //其中i为partition的编号，x为位于该partition的值
      b.foreach(x=>println(x))  
      b.foreachWith(i=>i)((x,i)=>println("[aIndex"+i+"]"+x)) 
      r.foreach(x=>println(x))  
      r.foreachWith(i=>i)((x,i)=>println("[aIndex"+i+"]"+x)) 
      println(r.toDebugString)    //将r的RDD调用过程打印出来
      sc.stop()
      
  }
}

```
>上述程序主要用来展示从scala集合中来构造RDD，主要两个方法：makeRDD和parallelize，其构造的scala集合必须是继承自Seq序列的一些数据容器：Array、List、Queue等。

第二种：**从文件系统中构建RDD**

```scala
package org.nita.sparkExample.spark
import org.apache.spark._
import SparkContext._
object HdfsWordCount {

 def main(args: Array[String]) {
    if (args.length < 1) {
      System.err.println("Usage: HdfsTest <file>")
      System.exit(1)
    }
    val sparkConf = new SparkConf().setAppName("HdfsTest").setMaster("local[4]")
                                    .set("spark.driver.allowMultipleContexts","true")
        
    val sc = new SparkContext(sparkConf)
    val file = sc.textFile(args(0))       //如果是本地文件：file:///home/havstack/xxx  如果是HDFS：hdfs://slave2:9000/data/test/xxx
    val result = file.flatMap(_.split(" ")).map(x => (x, 1)).reduceByKey(_ + _).cache() //map reduce操作
    result.foreach{x => println(x._1 + " " + x._2)}  //打印word和及其数量
    val sorted=result.map{
      case(key,value)=>(value,key)   //对（key，value）进行反转（value，key）
    }.sortByKey(true,1)          //对value进行排序
    val topk=sorted.top(10);     //取top10的word
    topk.foreach(println)        //打印
    sc.stop()                    //停止spark job
  }
}

```
>这里的主要算子有：textFile、wholeTextFiles、binaryFiles、binaryRecords、hadoopRDD、hadoopFile、newAPIHadoopFile、sequenceFile、objectFile、checkpointFile
这里分布简单介绍这些算子：

- **textFile**:textFile(path: String, minPartitions: Int = defaultMinPartitions):两个参数，一个是要处理的文件的路径名，一个是设置其最小partitions。用来处理文本文件。
- **wholeTextFiles**：wholeTextFiles(path: String, minPartitions: Int = defaultMinPartitions)，第一个参数是要处理的多个文件的公共路径，第二是最小partitions。主要用来处理多个文本文件。
- **binaryFiles**：binaryFiles(path: String, minPartitions: Int = defaultMinPartitions)，第一个参数是要处理的多个文件的公共路径，第二是最小partitions。主要用来多个处理二进制文件。
- **binaryRecords**：binaryRecords(path: String, recordLength: Int, conf: Configuration = hadoopConfiguration)，如果处理的文件的每条记录的长度都是一致的话，可以使用这个接口。
- **hadoopRDD**，这个方法很有用，如果你不是使用hadoop支持的文件格式，而是自己定了一个InputFormat用来处理你需要的处理的数据，比如视频文件如何处理，大家可以参考我写的**[cipher]**项目,里面实现了如何从hdfs上处理视频文件。
- **hadoopFile**，和hadoopRDD差不多
- **newAPIHadoopFile**和hadoopRDD差不多
- **sequenceFile**，用来支持hadoop sequenceFIle的接口，这里不详细介绍。
- **checkpointFile**，用来从checkpointFIle读取数据

以上只是对这些算子进行了简单的介绍，之后会选择几个典型的算子详细的分析器**内部实现**。

**（2）Value数据类型的Transformation算子**
> 处理数据类型为Value的RDD算子可以按着转换前数据分区与装换后数据分区的比例可以分为一下几类：
- 输入分区与输出分区一比一
- 输入分区与输出分区多比一
- 输入分区与输出分区一比多
- 输入分区与输出分区多比多

> 下面分别介绍

    1.输入分区与输出分区一比一
（1）map操作
map操作是对每个分区的每条记录进行一次计算，而最后为每个分区返回一个对象，这里主要注意和flatMap的区别。下图是map对RDD的逻辑转换图

![图](https://github.com/gjhkael/deployDoc/blob/master/image/map.PNG)

（2）flatMap操作
 flatMap也是对这个RDD里面的所有数据进行了一次计算，但是其返回的是一整个对象，其中将不同分区的计算结果进行了合并。这里如果是单词计数的话需要使用flatMap,因为map操作的结果是多个集合，没有办法进行合并操作。下图是flatMap对RDD的逻辑转换图
 
![图](https://github.com/gjhkael/deployDoc/blob/master/image/flatMap.PNG)

（3）mapPartitions操作
 mapPartitions函数获取到每个分区的迭代器，在函数中通过这个分区整体的迭代器对整个分区的元素进行操作。起RDD逻辑转换图和map一样，只是它是通过获得迭代器分区来操作数据。

（4）mapPartitionsWithIndex操作
这个操作和（3）不同的是它能获得每个分区的索引号，这样的话就能保证数据原始的顺序性，比如有些合并操作是要依赖原始数据的前后次序，那么这个算子很有作用。

（5）glom操作
glom是将每个分区形成一个数组.其逻辑图如下：

![图](https://github.com/gjhkael/deployDoc/blob/master/image/glom.PNG)
	
	2.输入分区与输出分区多比一
（1）union操作

（2）cartesian操作
	
	3.输入分区与输出分区一对多
（1）filter
	
	4.输入分区与输出分区多对多
	
	
	
**（3）Key-Value数据类型的Transformation算子**


**（4）Action类型的算子**

###RDD源码研究

**ParallelCollectionRDD**


**NewHadoopRDD**



[SparkInternals]:https://github.com/JerryLead/SparkInternals/blob/master/markdown/2-JobLogicalPlan.md
[cipher]:https://github.com/gjhkael/cipher
[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]:http://www.eecs.berkeley.edu/Pubs/TechRpts/2011/EECS-2011-82.pdf